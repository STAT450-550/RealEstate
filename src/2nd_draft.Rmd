---
title: "2nd_draft"
author: "Xuechun Lu, Yuting Wen, Peter Han, Yuetong Liu"
date: "15/03/2020"
output:
  github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
libraries
```{r, message=F, warning=F}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(here)
library(readxl)
require(neuralnet)
library(plotly)
```

1. data processing
```{r, warning=F, message=F}
## Aggregated Data
assessment_aggregate <- read.csv(here("data","assessment_aggregate.csv"))[,-1]
```

2. Test correlations between past mill rates and other features.
```{r}
# continuous variables
assessment_transform <- assessment_aggregate
## TODO: mean, log transform, etc.

for(i in colnames(assessment_transform)[4:7]){
  print(sprintf("correlation between mill rate and %s: %s.",
                i, cor(assessment_aggregate$rate, eval(parse(text=paste0("assessment_aggregate$", i))))))
  print(assessment_aggregate %>%
                   ggplot(aes(x=!!as.name(i),
                              y=rate,group=AddressAssessorMunicipalityDesc,
                              color=AddressAssessorMunicipalityDesc)) + 
                   geom_point() + 
                   ggtitle(sprintf("mill rate vs %s ", i)) + 
                   geom_smooth(aes(group = 1), size = 0.5, method = "lm", se = FALSE, colour = "black"))

}

# categorical variables
for(i in colnames(assessment_transform)[1:3]){
  print(ggplot(assessment_aggregate, aes(x=as.factor(!!as.name(i)),y=rate, fill=as.factor(!!as.name(i)))) + 
  geom_boxplot()+ 
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = "none"))
}
```


3.model fitting
(i) linear model
```{r}
linear_full<-lm(rate~factor(AddressAssessorMunicipalityDesc)+factor(Year)+factor(TaxClassCode)+assessTotal+landTotal+improvementTotal+propertyCount,data=assessment_aggregate)
summary(linear_full)


library(broom)

linear_full_fit<-augment(linear_full)
sqrt(sum((linear_full_fit$.resid)^2)/nrow(assessment_aggregate)) 
```
Multiple R-squared:  0.8874,	Adjusted R-squared:  0.8707 
MSPE =  1.984285

Next: variable selection
with only significant variables
```{r}
reduced<-lm(rate~factor(Year)+factor(TaxClassCode)+factor(AddressAssessorMunicipalityDesc)+assessTotal+landTotal, data=assessment_aggregate)
summary(reduced)

reduced_fit<-augment(reduced)
sqrt(sum((reduced_fit$.resid)^2)/nrow(assessment_aggregate))
```
Multiple R-squared:  0.8874,	Adjusted R-squared:  0.8721 
MSPE = 1.984541





(ii) Ridge, Lasso and elastic net.

Ridge Regression
reference: https://www.pluralsight.com/guides/linear-lasso-and-ridge-regression-with-r
```{r}
library(glmnet)
library(dummies)
dummy_year<-dummy(assessment_aggregate$Year)
dummy_municipal<-dummy(assessment_aggregate$AddressAssessorMunicipalityDesc)
dummy_taxclass<-dummy(assessment_aggregate$TaxClassCode)
# build x matrix
x<-cbind(dummy_municipal,dummy_year,dummy_taxclass,assessment_aggregate$assessTotal,assessment_aggregate$landTotal,assessment_aggregate$improvementTotal,assessment_aggregate$propertyCount,assessment_aggregate$tax)

y<-assessment_aggregate$rate
lambdas <- 10^seq(2, -3, by = -.1)
dim(x)


lambdas <- 10^seq(2, -3, by = -.1)
ridge_reg = glmnet(x, y, nlambda = 25, alpha = 0, family = 'gaussian', lambda = lambdas)
set.seed(450)
cv_ridge <- cv.glmnet(x, y, alpha = 0, lambda = lambdas, nfolds=10)
optimal_lambda <- cv_ridge$lambda.min
optimal_lambda
predictions_train <- predict(ridge_reg, s = optimal_lambda, newx = x)

# Compute R^2 from true and predicted values
eval_results <- function(true, predicted) {
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- 1 - SSE / SST
  MSPE = sqrt(SSE/nrow(predicted))
# Model performance metrics
data.frame(
  MSPE = MSPE,
  Rsquare = R_square
)
  
}

predictions_train <- predict(ridge_reg, s = optimal_lambda, newx = x)
ridge_r <- eval_results(y, predictions_train)


```
MSPE = 1.989583
Rsquare=	0.8868242	


Lasso 
```{r}
# Setting alpha = 1 implements lasso regression
set.seed(450)
lasso_reg <- cv.glmnet(x, y, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 10)

# Best 
lambda_best <- lasso_reg$lambda.min;lambda_best

lasso_model <- glmnet(x, y, alpha = 1, lambda = lambda_best, standardize = TRUE)

predictions_train <- predict(lasso_model, s = lambda_best, newx = x)
lasso_r <- eval_results(y, predictions_train)
```
MSPE = 1.985463	
Rsquare=0.8872924	


Elastic net: 
reference: https://daviddalpiaz.github.io/r4sl/elastic-net.html
```{r}
library(caret)
tibble::as_tibble(assessment_aggregate)
cv_10 = trainControl(method = "cv", number = 10)
elastic_net = train(
  rate~factor(AddressAssessorMunicipalityDesc)+factor(Year)+factor(TaxClassCode)+assessTotal+landTotal+improvementTotal+propertyCount, data = assessment_aggregate,
  method = "glmnet",
  trControl = cv_10
)

elastic_net

```
The final values used for the model were alpha = 0.1 and lambda = 0.007102922.
   alpha  lambda       RMSE      Rsquared   MAE     
  0.10   0.007102922  2.236598  0.8625398  1.708885


Prediction power
train/test
```{r}
set.seed(450)
train_ind<-sample(218,218-50)
train<-assessment_aggregate[train_ind,]
test<-assessment_aggregate[-train_ind,]
```

Full linear model
```{r}

newx<-test[,-c(8,9)]
y<-test[,c(8)]
linear_1<-lm(rate~factor(AddressAssessorMunicipalityDesc)+factor(Year)+factor(TaxClassCode)+assessTotal+landTotal+improvementTotal+propertyCount,data=train)
resid<-predict(linear_1,newdata = newx) - y
sqrt(sum(resid^2)/nrow(test))
```
MSPE =  2.590233

Reduced model
```{r}
# reduced variable
linear_2<-lm(rate~factor(Year)+factor(TaxClassCode)+factor(AddressAssessorMunicipalityDesc)+assessTotal+landTotal,data=train)
resid<-predict(linear_2,newdata = newx) - y
sqrt(sum(resid^2)/nrow(test))
```
MSPE = 2.523658

Lasso
```{r}
# create the whole matrix
y<-as.matrix(assessment_aggregate$rate)
dim(x) # 165  29
dim(y)
# creat x_train matrix and y_train
x_train<-x[train_ind,]
y_train<-y[train_ind,]
# create x_test matrix
x_test<-x[-train_ind,]
y_test<-y[-train_ind,]

# Setting alpha = 1 implements lasso regression
set.seed(450)
lasso_reg <- cv.glmnet(x_train, y_train, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 10)

# Best 
lambda_best <- lasso_reg$lambda.min;lambda_best

lasso_model <- glmnet(x_train, y_train, alpha = 1, lambda = lambda_best, standardize = TRUE)

predictions_test <- predict(lasso_model, s = lambda_best, newx = x_test)
eval_results(y_test, predictions_test)
```
MSPE = 2.528047	


Ridge
```{r}
ridge_reg = glmnet(x_train, y_train, nlambda = 25, alpha = 0, family = 'gaussian', lambda = lambdas)
set.seed(450)
cv_ridge <- cv.glmnet(x_train, y_train, alpha = 0, lambda = lambdas, nfolds=10)
optimal_lambda <- cv_ridge$lambda.min
optimal_lambda
predictions_test <- predict(ridge_reg, s = optimal_lambda, newx = x_test)
eval_results(y_test, predictions_test)
```
MSPE = 2.567499	

Elastic Net
```{r}
tibble::as_tibble(assessment_aggregate[train_ind,])
cv_10 = trainControl(method = "cv", number = 10)
elastic_net = train(
 rate~factor(AddressAssessorMunicipalityDesc)+factor(Year)+factor(TaxClassCode)+assessTotal+landTotal+improvementTotal+propertyCount, 
 data = assessment_aggregate[train_ind,],
  method = "glmnet",
  trControl = cv_10
)
elastic_net

# RMSE was used to select the optimal model using the smallest value.
# The final values used for the model were alpha = 1 and lambda = 0.06549203.

elastic_reg = glmnet(x_train, y_train, nlambda = 25, alpha = 1, family = 'gaussian', lambda =  0.06549203)
predictions_test <- predict(elastic_reg, newx = x_test)
eval_results(y_test, predictions_test)
```
MSPE = 2.54861	

Summary on GOF
```{r}

GOF_table <- rbind(
      c("OLR full", 0.8874, 0.8707, 1.9843, 2.5902),
      c("OLR reduced", 0.8874, 0.8721, 1.9845, 2.5237),
      c("Ridge", 0.8868, 0.8707, 1.9896, 2.5675),
      c("LASSO", 0.8873, 0.8707, 1.9855, 2.5280),
      c("Elastic Net", 0.8625, 0.8707, 2.2366, 2.5486))
colnames(GOF_table) <- c("Model", "Mutiple R-Squared","Adjusted R_Squared", "MSE", "PMSE")

```

```{r}
dummy_year<-dummy(assessment_aggregate$Year)
dummy_municipal<-dummy(assessment_aggregate$AddressAssessorMunicipalityDesc)
dummy_taxclass<-dummy(assessment_aggregate$TaxClassCode)
# build x matrix
x<-cbind(dummy_municipal,dummy_year,dummy_taxclass,assessment_aggregate$assessTotal,assessment_aggregate$landTotal,assessment_aggregate$improvementTotal,assessment_aggregate$propertyCount,assessment_aggregate$tax)

y<-assessment_aggregate$rate
x<-cbind(x,y)
# correct col name
#assessTotal,assessment_aggregate$landTotal,assessment_aggregate$improvementTotal,assessment_aggregate$propertyCount,assessment_aggregate$tax
colnames(x)[28:32]<-c("assessTotal","landTotal","improvementTotal","propertyCount","tax")
xm<-x[,1:(ncol(x)-1)]
x.data.frame<-assessment_aggregate
xm.data.frame<-assessment_aggregate[,-8]
y.data.frame<-assessment_aggregate$rate
n <- nrow(xm)
k <- 10
ii <- (1:n)%%k + 1
set.seed(123)
N <- 50
mspe.el<-mspe.la <- mspe.f <- mspe.ri <- mspe.reduced <- rep(0, N)

for (i in 1:N) {
  ii <- sample(ii)
  pr.el<-pr.la <- pr.f <- pr.ri <- pr.reduced <- rep(0, n)
  for (j in 1:k) {
    tmp.ri <- cv.glmnet(x = xm[ii != j, ], y = y[ii != j], lambda = lambdas, 
                        nfolds = 10, alpha = 0, family = "gaussian")
    tmp.la <- cv.glmnet(x = xm[ii != j, ], y = y[ii != j], lambda = lambdas, 
                        nfolds = 10, alpha = 1, family = "gaussian")
    tmp.reduced <- lm(rate ~ factor(Year)+factor(TaxClassCode)+factor(AddressAssessorMunicipalityDesc)+assessTotal+landTotal, 
                  data = x.data.frame[ii != j, ])
    tmp.full <- lm(rate ~ factor(AddressAssessorMunicipalityDesc)+factor(Year)+factor(TaxClassCode)+assessTotal+landTotal+improvementTotal+propertyCount+tax, 
               data = x.data.frame[ii != j, ])
    
    tmp.elastic<-train(
      rate~factor(AddressAssessorMunicipalityDesc)+factor(Year)+factor(TaxClassCode)+assessTotal+landTotal+improvementTotal+propertyCount+tax, 
      data = x.data.frame[ii != j, ],
      method = "glmnet",
      trControl = cv_10
    )
    
    pr.ri[ii == j] <- predict(tmp.ri, s = tmp.ri$lambda.min, newx = xm[ii == 
                                                                    j, ])
    pr.la[ii == j] <- predict(tmp.la, s = tmp.la$lambda.min, newx = xm[ii == 
                                                                    j, ])
    pr.reduced[ii == j] <- predict(tmp.reduced, newdata = x.data.frame[ii == j, ])
    pr.f[ii == j] <- predict(tmp.full, newdata = x.data.frame[ii == j, ])
    
    index<-which.min(tmp.elastic$results$RMSE)
    pr.el[ii == j] <- predict(tmp.elastic, 
                              alpha = tmp.elastic$results[index,1], 
                              lambda = tmp.elastic$results[index,2], 
                              newdata = xm.data.frame[ii == j, ])
  }
  mspe.ri[i]<-mean((y - pr.ri)^2)
  mspe.la[i]<-mean((y - pr.la)^2)
  mspe.reduced[i]<-mean((y - pr.reduced)^2)
  mspe.f[i]<-mean((y - pr.f)^2)
  mspe.el[i]<-mean((y - pr.el)^2)
  
}
boxplot(mspe.la, mspe.ri,mspe.el, mspe.reduced, mspe.f, 
        names = c("LASSO", "Ridge", "Elastic Net", "Reduced", "Full"), 
        col = c("yellow","steelblue", "gray80", "tomato", "springgreen"), 
        cex.axis = 1, 
        cex.lab = 1, 
        cex.main = 2)
mtext(expression(hat(MSPE)), side = 2, line = 2.5)

```
